{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create net\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, device, weight_init = 'xavier', num_filters = (16,32)):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.device = device\n",
    "        self.weight_init = weight_init\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=num_filters[0],\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2,\n",
    "            ),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=num_filters[0], \n",
    "                out_channels=num_filters[1], \n",
    "                kernel_size=5, \n",
    "                stride=1, \n",
    "                padding=2\n",
    "            ),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Sequential(nn.Linear(num_filters[1] * 7 * 7, 10),\n",
    "                                 nn.Softmax())\n",
    "        #self.apply(self.init_wieghts)\n",
    "\n",
    "    \n",
    "    def init_wieghts(self, w):\n",
    "        if isinstance(w, nn.Conv2d) or isinstance(w, nn.Linear):\n",
    "            if self.weight_init == 'xavier':\n",
    "                torch.nn.init.xavier_normal_(w.weight, gain = nn.init.calculate_gain('sigmoid'))\n",
    "            if self.weight_init == 'he':\n",
    "                torch.nn.init.kaiming_normal_(w.weight, nonlinearity='sigmoid')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.is_tensor(x):    \n",
    "            x = x.to(self.device)\n",
    "        else:\n",
    "            x = torch.from_numpy(x).to(self.device)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "  from mnist_loader import load_data_wrapper\n",
    "  train, val, test = load_data_wrapper()\n",
    "  train_x, train_y = zip(*train)\n",
    "  val_x, val_y = zip(*val)\n",
    "  test_x, test_y = zip(*test)\n",
    "  train_x_ , val_x_, test_x_ = [], [] ,[]\n",
    "\n",
    "  for t in train_x:\n",
    "    train_x_.append(t.reshape((1,28,28)))\n",
    "  for v ,ts in zip(val_x, test_x):\n",
    "    val_x_.append(v.reshape((1,28,28)))\n",
    "    test_x_.append(ts.reshape((1,28,28)))\n",
    "  \n",
    "\n",
    "  train_x_ , val_x_, test_x_ = np.array(train_x_), np.array(val_x_), np.array(test_x_) \n",
    " \n",
    "  train_y_ =  np.array([np.argmax(np.squeeze(t), axis=0) for t in train_y])\n",
    "  val_y_ = np.array(val_y)\n",
    "  test_y_ = np.array(test_y)\n",
    "  \n",
    "\n",
    "  return (train_x_, train_y_), (val_x_, val_y_), (test_x_, test_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CustomMnistDataset(Dataset):\n",
    "  def __init__(self, X,Y):\n",
    "    super().__init__()\n",
    "    self.X = np.divide(X, 255.0)\n",
    "    self.Y = Y\n",
    "  def __len__(self):\n",
    "    return len(self.Y)\n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.Y[index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, loss_fn, epoch_index, device):\n",
    "  loss = 0\n",
    "  for i, batch in enumerate(train_loader):\n",
    "    x, y = batch\n",
    "    y = y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  print(f'Training Loss for last batch of epoch {epoch_index}: {loss}')\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH #0\n",
      "tensor([0.0928, 0.0818, 0.1163, 0.0998, 0.1112, 0.1028, 0.0946, 0.0925, 0.1003,\n",
      "        0.1079], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([8, 8, 9, 6, 3, 8, 5, 7], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Benjamin\\OneDrive - University of Virginia\\Third Year\\CS 6336- Machine Learning\\final project\\CS6336\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch (got input: [10], target: [8])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Benjamin\\OneDrive - University of Virginia\\Third Year\\CS 6336- Machine Learning\\final project\\CS6336\\neural_network_pytorch.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEPOCH #\u001b[39m\u001b[39m{\u001b[39;00mEPOCH\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#X13sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#X13sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m train_epoch(model, train_dataloader, optimizer, loss_fn, EPOCH, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#X13sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[1;32mc:\\Users\\Benjamin\\OneDrive - University of Virginia\\Third Year\\CS 6336- Machine Learning\\final project\\CS6336\\neural_network_pytorch.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(y_pred)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(y)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_pred, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Benjamin\\OneDrive - University of Virginia\\Third Year\\CS 6336- Machine Learning\\final project\\CS6336\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Benjamin\\OneDrive - University of Virginia\\Third Year\\CS 6336- Machine Learning\\final project\\CS6336\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Benjamin\\OneDrive - University of Virginia\\Third Year\\CS 6336- Machine Learning\\final project\\CS6336\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\Benjamin\\OneDrive - University of Virginia\\Third Year\\CS 6336- Machine Learning\\final project\\CS6336\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch (got input: [10], target: [8])"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from  torcheval.metrics.functional import multiclass_accuracy, multiclass_f1_score, multiclass_precision, multiclass_recall, auc\n",
    "#training code\n",
    "\n",
    "#hyperparams\n",
    "hyper_params = {\n",
    "  \"EPOCHS\": 25,\n",
    "  \"OPTIMIZER\": 'adam',\n",
    "  \"NUM_FILTERS\": (32,64),\n",
    "  \"WEIGHT_INIT\": 'xavier',\n",
    "  \"BATCH_SIZE\": 8\n",
    "}\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "model = ConvNet(device=device,weight_init=hyper_params[\"WEIGHT_INIT\"],num_filters=hyper_params[\"NUM_FILTERS\"])\n",
    "model.to(device)\n",
    "\n",
    "#create loss func and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= .01)\n",
    "\n",
    "train, (val_x, val_y), _ = get_mnist()\n",
    "val_y = torch.from_numpy(val_y).to(device)\n",
    "train_dataloader = DataLoader(CustomMnistDataset(train[0],train[1]),batch_size=hyper_params[\"BATCH_SIZE\"],shuffle=True)\n",
    "\n",
    "for EPOCH in range(hyper_params[\"EPOCHS\"]):\n",
    "  print(f'EPOCH #{EPOCH}')\n",
    "  model.train()\n",
    "  train_epoch(model, train_dataloader, optimizer, loss_fn, EPOCH, device)\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    val_y_pred = model(val_x)\n",
    "    v_loss = loss_fn(val_y_pred, val_y)\n",
    "    v_acc = multiclass_accuracy(val_y_pred, val_y, num_classes=10)\n",
    "    v_f1 = multiclass_f1_score(val_y_pred, val_y, num_classes=10)\n",
    "    v_prec = multiclass_precision(val_y_pred, val_y, num_classes=10)\n",
    "    v_recall = multiclass_recall(val_y_pred, val_y, num_classes=10)\n",
    "    #v_auc = auc(val_y_pred, val_y)\n",
    "  if EPOCH == 10:\n",
    "    print(val_y[50:70])\n",
    "    print(np.array([np.argmax(np.squeeze(t), axis=0) for t in val_y_pred.cpu()])[50:70])\n",
    "  print(f'Validation Acc: {v_acc} | Loss: {v_loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4859\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "train, (val_x, val_y), _ = get_mnist()\n",
    "count = 0\n",
    "for i in train[1]:\n",
    "  if i ==4:\n",
    "    count+=1\n",
    "\n",
    "print(count)\n",
    "print(len(train[1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
