{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create net\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, device, weight_init = 'xavier', num_filters = (16,32)):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.device = device\n",
    "        self.weight_init = weight_init\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=num_filters[0],\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2,\n",
    "            ),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=num_filters[0], \n",
    "                out_channels=num_filters[1], \n",
    "                kernel_size=5, \n",
    "                stride=1, \n",
    "                padding=2\n",
    "            ),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Sequential(nn.Linear(num_filters[1] * 7 * 7, 10))#,\n",
    "                                # nn.Softmax())\n",
    "        self.apply(self.init_wieghts)\n",
    "\n",
    "    \n",
    "    def init_wieghts(self, w):\n",
    "        if isinstance(w, nn.Conv2d) or isinstance(w, nn.Linear):\n",
    "            torch.nn.init.normal_(w.weight,0,.01)\n",
    "            if self.weight_init == 'xavier':\n",
    "                torch.nn.init.xavier_normal_(w.weight, gain = nn.init.calculate_gain('sigmoid'))\n",
    "            if self.weight_init == 'he':\n",
    "                torch.nn.init.kaiming_normal_(w.weight, nonlinearity='sigmoid')\n",
    "            if self.weight_init == 'normal':\n",
    "                torch.nn.init.normal_(w.weight,0,.01)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.is_tensor(x):    \n",
    "            x = x.to(self.device)\n",
    "        else:\n",
    "            x = torch.from_numpy(x).to(self.device)\n",
    "        x.requires_grad_()\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "  from mnist_loader import load_data_wrapper\n",
    "  train, val, test = load_data_wrapper()\n",
    "  train_x, train_y = zip(*train)\n",
    "  val_x, val_y = zip(*val)\n",
    "  test_x, test_y = zip(*test)\n",
    "  train_x_ , val_x_, test_x_ = [], [] ,[]\n",
    "\n",
    "  for t in train_x:\n",
    "    train_x_.append(t.reshape((1,28,28)))\n",
    "  for v ,ts in zip(val_x, test_x):\n",
    "    val_x_.append(v.reshape((1,28,28)))\n",
    "    test_x_.append(ts.reshape((1,28,28)))\n",
    "  \n",
    "\n",
    "  train_x_ , val_x_, test_x_ = np.array(train_x_), np.array(val_x_), np.array(test_x_) \n",
    " \n",
    "  train_y_ =  np.array([np.argmax(np.squeeze(t), axis=0) for t in train_y])\n",
    "  val_y_ = np.array(val_y)\n",
    "  test_y_ = np.array(test_y)\n",
    "  \n",
    "\n",
    "  return (train_x_, train_y_), (val_x_, val_y_), (test_x_, test_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "class CustomMnistDataset(Dataset):\n",
    "  def __init__(self, X,Y, transform = None):\n",
    "    super().__init__()\n",
    "    self.X = np.divide(X, 255.0)\n",
    "    self.Y = Y\n",
    "    self.transform = transform\n",
    "  def __len__(self):\n",
    "    return len(self.Y)\n",
    "  def __getitem__(self, index):\n",
    "    x = self.X[index]\n",
    "    x = Image.fromarray(x, mode=\"L\")\n",
    "    y = self.Y[index]\n",
    "    if self.transform:\n",
    "      x = self.transform(x)\n",
    "      x = torch.permute(x, (1, 0, 2))\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, loss_fn, epoch_index, device):\n",
    "  loss = 0\n",
    "  for i, batch in enumerate(train_loader):\n",
    "    x, y = batch\n",
    "    y = y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  print(f'Training Loss for last batch of epoch {epoch_index}: {loss}')\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "EPOCH #0\n",
      "Training Loss for last batch of epoch 0: 0.0026201752480119467\n",
      "Validation Acc: 0.9637999534606934 | Loss: 1.1155998706817626e-05\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ./results/mnist_models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Benjamin\\OneDrive - University of Virginia\\Third Year\\CS 6336- Machine Learning\\final project\\CS6336\\neural_network_pytorch.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#W5sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     \u001b[39m#v_auc = auc(val_y_pred, val_y)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#W5sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m   \u001b[39m#if EPOCH == 10:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#W5sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     \u001b[39m#print(val_y[50:70])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#W5sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     \u001b[39m#print(np.array([np.argmax(np.squeeze(t), axis=0) for t in val_y_pred.cpu()])[50:70])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#W5sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation Acc: \u001b[39m\u001b[39m{\u001b[39;00mmetrics[\u001b[39m\"\u001b[39m\u001b[39mv_accuracy\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m | Loss: \u001b[39m\u001b[39m{\u001b[39;00mmetrics[\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#W5sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m   torch\u001b[39m.\u001b[39;49msave(model\u001b[39m.\u001b[39;49mstate_dict(), model_directory\u001b[39m+\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mepoch_\u001b[39;49m\u001b[39m{\u001b[39;49;00mEPOCH\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#W5sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mprint\u001b[39m(metrics)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Benjamin/OneDrive%20-%20University%20of%20Virginia/Third%20Year/CS%206336-%20Machine%20Learning/final%20project/CS6336/neural_network_pytorch.ipynb#W5sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(metrics, index\u001b[39m=\u001b[39mmetrics[\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Benjamin\\OneDrive - University of Virginia\\Third Year\\CS 6336- Machine Learning\\final project\\CS6336\\.venv\\lib\\site-packages\\torch\\serialization.py:618\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    615\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    617\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 618\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    620\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Benjamin\\OneDrive - University of Virginia\\Third Year\\CS 6336- Machine Learning\\final project\\CS6336\\.venv\\lib\\site-packages\\torch\\serialization.py:492\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 492\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[1;32mc:\\Users\\Benjamin\\OneDrive - University of Virginia\\Third Year\\CS 6336- Machine Learning\\final project\\CS6336\\.venv\\lib\\site-packages\\torch\\serialization.py:463\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39mPyTorchFileWriter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_stream))\n\u001b[0;32m    462\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory ./results/mnist_models does not exist."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from  torcheval.metrics.functional import multiclass_accuracy, multiclass_f1_score, multiclass_precision, multiclass_recall, auc\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd\n",
    "import json\n",
    "#training code\n",
    "\n",
    "#hyperparams\n",
    "hyper_params = {\n",
    "  \"EPOCHS\": 20,\n",
    "  \"OPTIMIZER\": 'adam',\n",
    "  \"NUM_FILTERS\": (32,64),\n",
    "  \"WEIGHT_INIT\": 'normal',\n",
    "  \"BATCH_SIZE\": 8,\n",
    "  \"LEARNING_RATE\": 0.001\n",
    "}\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "model = ConvNet(device=device,weight_init=hyper_params[\"WEIGHT_INIT\"],num_filters=hyper_params[\"NUM_FILTERS\"])\n",
    "model.to(device)\n",
    "model_directory = './results/mnist_models/'\n",
    "\n",
    "\n",
    "#create loss func and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= hyper_params['LEARNING_RATE'])\n",
    "\n",
    "'''train, (val_x, val_y), _ = get_mnist()\n",
    "val_y = torch.from_numpy(val_y).to(device)\n",
    "train_dataloader = DataLoader(CustomMnistDataset(train[0],train[1], transform=ToTensor()),batch_size=hyper_params[\"BATCH_SIZE\"],shuffle=True)\n",
    "'''\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(), \n",
    "    download = True,            \n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,                         \n",
    "    transform = ToTensor(), \n",
    "    download = True,            \n",
    ")\n",
    "train_data, val_data = random_split(train_data, lengths=(5/6,1/6))\n",
    "val_loader = DataLoader(val_data, batch_size=len(val_data))\n",
    "val_x, val_y = next(iter(val_loader))\n",
    "val_y = val_y.to(device)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, \n",
    "                                          batch_size=hyper_params['BATCH_SIZE'], \n",
    "                                          shuffle=True)\n",
    "\n",
    "\n",
    "metrics= {\n",
    "  'epoch': [],\n",
    "  'train_loss': [],\n",
    "  'val_loss': [],\n",
    "  'v_accuracy': [],\n",
    "  'v_precision': [],\n",
    "  'v_recall': [],\n",
    "  'v_f1': []\n",
    "}\n",
    "for EPOCH in range(hyper_params[\"EPOCHS\"]):\n",
    "  print(f'EPOCH #{EPOCH}')\n",
    "  model.train()\n",
    "  train_loss = train_epoch(model, train_dataloader, optimizer, loss_fn, EPOCH, device)/hyper_params['BATCH_SIZE']\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    val_y_pred = model(val_x)\n",
    "    metrics['epoch'].append(EPOCH)\n",
    "    metrics['val_loss'].append(loss_fn(val_y_pred, val_y).item()/ len(val_y))\n",
    "    metrics['train_loss'].append(train_loss.item() / hyper_params[\"BATCH_SIZE\"])\n",
    "    metrics['v_accuracy'].append(multiclass_accuracy(val_y_pred, val_y, num_classes=10).item())\n",
    "    metrics['v_f1'].append(multiclass_f1_score(val_y_pred, val_y, num_classes=10).item())\n",
    "    metrics['v_precision'].append(multiclass_precision(val_y_pred, val_y, num_classes=10).item())\n",
    "    metrics['v_recall'].append(multiclass_recall(val_y_pred, val_y, num_classes=10).item())\n",
    "    #v_auc = auc(val_y_pred, val_y)\n",
    "  #if EPOCH == 10:\n",
    "    #print(val_y[50:70])\n",
    "    #print(np.array([np.argmax(np.squeeze(t), axis=0) for t in val_y_pred.cpu()])[50:70])\n",
    "  print(f'Validation Acc: {metrics[\"v_accuracy\"][-1]} | Loss: {metrics[\"val_loss\"][-1]}')\n",
    "  torch.save(model.state_dict(), model_directory+f'epoch_{EPOCH}.pt')\n",
    "\n",
    "print(metrics)\n",
    "df = pd.DataFrame(metrics, index=metrics['epoch'])\n",
    "df.to_csv('./results/mnist.csv')\n",
    "\n",
    "with open('./results/mnist_params.json', 'w') as f:\n",
    "    json.dump(hyper_params, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#training code\n",
    "\n",
    "#hyperparams\n",
    "hyper_params = {\n",
    "  \"EPOCHS\": 32,\n",
    "  \"OPTIMIZER\": 'adam',\n",
    "  \"NUM_FILTERS\": (32,64),\n",
    "  \"WEIGHT_INIT\": 'normal',\n",
    "  \"BATCH_SIZE\": 8,\n",
    "  \"LEARNING_RATE\": 0.001\n",
    "}\n",
    "\n",
    "with open('./results/mnist_params.json', 'w') as f:\n",
    "  json.dump(hyper_params, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
